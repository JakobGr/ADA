{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["q18cPGRUSqcs","zGF0WixIUG8B","ELMDJLCtURyk","L1h6__VlVwnq"],"authorship_tag":"ABX9TyMZlPf2qFr6JQG0C5muNiTa"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"IWA3ia4gSJ-U"},"outputs":[],"source":["# Libraries\n","import torch\n","import torch.nn as nn\n","from transformers import AutoProcessor, VideoMAEForVideoClassification\n","from torch.utils.data import DataLoader, Dataset, ConcatDataset\n","from tqdm import tqdm\n","import json\n","import os\n","import random\n","import numpy as np\n","import cv2\n","from torchvision import transforms\n","from torch.optim import AdamW\n","from transformers import get_linear_schedule_with_warmup\n","from torch.cuda.amp import GradScaler, autocast"]},{"cell_type":"markdown","source":["## VideoMEA: 20 or 30 Epochs with Even Frames"],"metadata":{"id":"q18cPGRUSqcs"}},{"cell_type":"code","source":["# Step 1: Load Annotations and Determine Number of Classes\n","annotation_file = 'nslt_100.json'\n","root_dir = 'WLASL100_videos'\n","with open(annotation_file, \"r\") as f:\n","    annotations = json.load(f)\n","\n","# Ensure that gloss_to_index contains mappings for all unique labels in the dataset\n","all_actions = set(data[\"action\"][0] for data in annotations.values())\n","gloss_to_index = {action: idx for idx, action in enumerate(sorted(all_actions))}\n","num_classes = len(gloss_to_index)\n","\n","# Create the inverse mapping from index to gloss\n","index_to_gloss = {idx: gloss for gloss, idx in gloss_to_index.items()}\n","\n","# Step 2: Initialize Model and Processor\n","pretrained_weights = \"MCG-NJU/videomae-base-finetuned-kinetics\"\n","processor = AutoProcessor.from_pretrained(pretrained_weights)\n","model = VideoMAEForVideoClassification.from_pretrained(\n","    pretrained_weights,\n","    num_labels=num_classes,  # Ensure model output fits dataset\n","    ignore_mismatched_sizes=True\n",")\n","\n","# Modify the model's classification head\n","model.classifier = nn.Sequential(\n","    nn.Dropout(p=0.5),\n","    nn.Linear(model.config.hidden_size, num_classes)  # Change to match num_classes\n",")\n","\n","# Detect device to execute model\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model.to(device)\n","\n","# Step 3: Implement Dataset Class\n","class VideoDataset(Dataset):\n","    def __init__(self, annotation_file, root_dir, processor, num_frames, transform, subset):\n","        self.annotation_file = annotation_file\n","        self.root_dir = root_dir\n","        self.processor = processor\n","        self.num_frames = num_frames\n","        self.transform = transform\n","        self.subset = subset\n","\n","        # Load annotations\n","        with open(self.annotation_file, \"r\") as f:\n","            self.annotations = json.load(f)\n","\n","        # Filter videos by subset\n","        self.video_list = [\n","            vid for vid, data in self.annotations.items() if data.get(\"subset\") == self.subset\n","        ]\n","        self.labels = {vid: data[\"action\"][0] for vid, data in self.annotations.items()}\n","\n","    def __len__(self):\n","        return len(self.video_list)\n","\n","    def __getitem__(self, idx):\n","        video_id = self.video_list[idx]\n","        label = gloss_to_index[self.labels[video_id]]\n","        video_path = os.path.join(self.root_dir, f\"{video_id}.mp4\")\n","        # Load video frames\n","        frames = self.load_video_frames(video_path)\n","        # Fallback for videos with no frames\n","        if not frames:\n","            print(f\"Warning: No frames loaded for video {video_id}.\")\n","            frames = [np.zeros((224, 224, 3), dtype=np.uint8) for _ in range(self.num_frames)]\n","        # Apply transformations if any\n","        if self.transform:\n","            frames = self.apply_transforms(frames)\n","        # Process frames with processor\n","        inputs = self.processor(frames, return_tensors=\"pt\", do_rescale=False)[\"pixel_values\"]\n","        return {\"pixel_values\": inputs.squeeze(0), \"labels\": torch.tensor(label)}\n","\n","    def load_video_frames(self, video_path):\n","        cap = cv2.VideoCapture(video_path)\n","        if not cap.isOpened():\n","            print(f\"Error: Cannot open video file {video_path}.\")\n","            return []\n","\n","        frames = []\n","        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n","\n","        # Calculate step size to evenly distribute frames\n","        if total_frames > 1:\n","            step = max(1, total_frames // self.num_frames)\n","        else:\n","            step = 1\n","\n","        for i in range(0, step * self.num_frames, step):\n","            cap.set(cv2.CAP_PROP_POS_FRAMES, i)\n","            ret, frame = cap.read()\n","            if not ret:\n","                break\n","            h, w, _ = frame.shape\n","            if min(w, h) < 226:\n","                scale = 226 / min(w, h)\n","                frame = cv2.resize(frame, None, fx=scale, fy=scale)\n","            if max(w, h) > 256:\n","                scale = 256 / max(w, h)\n","                frame = cv2.resize(frame, None, fx=scale, fy=scale)\n","            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n","            frames.append(frame)\n","\n","        cap.release()\n","\n","        # Handle cases with fewer frames than expected\n","        if len(frames) < self.num_frames:\n","            pad_choice = random.random()\n","            if pad_choice < 0.5 and frames:\n","                pad_frame = frames[0]\n","                while len(frames) < self.num_frames:\n","                    frames.insert(0, pad_frame)\n","            elif frames:\n","                pad_frame = frames[-1]\n","                while len(frames) < self.num_frames:\n","                    frames.append(pad_frame)\n","\n","        # In case frames were empty\n","        while len(frames) < self.num_frames:\n","            frames.append(np.zeros((224, 224, 3), dtype=np.uint8))\n","\n","        return frames\n","\n","    # consistent transformation for a video\n","    def apply_transforms(self, frames):\n","        \"\"\"\n","        Apply consistent transforms to all frames in a video.\n","        \"\"\"\n","        if self.subset == \"train\" or self.subset == \"val\":\n","            # Generate a single random crop and flip decision for the entire video\n","            crop_transform = transforms.RandomCrop(224)\n","            flip_transform = transforms.RandomHorizontalFlip(p=0.5)\n","            random_flip = random.random() < 0.5  # Determine if flipping is applied\n","\n","            # Define a manual crop region using RandomCrop's get_params\n","            first_frame = transforms.ToPILImage()(frames[0])\n","            i, j, h, w = transforms.RandomCrop.get_params(first_frame, output_size=(224, 224))\n","\n","            def consistent_transform(frame):\n","                frame = transforms.ToPILImage()(frame)\n","                frame = transforms.functional.crop(frame, i, j, h, w)  # Apply the same crop\n","                if random_flip:\n","                    frame = transforms.functional.hflip(frame)  # Apply the same flip\n","                return transforms.ToTensor()(frame)\n","\n","            # Apply the consistent transforms to all frames\n","            return [consistent_transform(frame) for frame in frames]\n","        else:  # For validation and testing\n","            pil_transforms = transforms.Compose([\n","                transforms.ToPILImage(),\n","                transforms.CenterCrop(224),\n","                transforms.ToTensor(),\n","            ])\n","            return [pil_transforms(frame) for frame in frames]\n","\n","# Step 4: Initialize Datasets and DataLoaders with Combined Training and Validation\n","# Create separate datasets\n","train_dataset = VideoDataset(\n","    annotation_file=annotation_file,\n","    root_dir=root_dir,\n","    processor=processor,\n","    num_frames=16,\n","    transform=True, # handled by transfrom function\n","    subset=\"train\"\n",")\n","val_dataset = VideoDataset(\n","    annotation_file=annotation_file,\n","    root_dir=root_dir,\n","    processor=processor,\n","    num_frames=16,\n","    transform=True, # handled by transfrom function\n","    subset=\"val\"\n",")\n","test_dataset = VideoDataset(\n","    annotation_file=annotation_file,\n","    root_dir=root_dir,\n","    processor=processor,\n","    num_frames=16,\n","    transform=True, # handled by transfrom function\n","    subset=\"test\"\n",")\n","\n","# Combine train and val datasets for training\n","combined_train_dataset = ConcatDataset([train_dataset, val_dataset])\n","\n","# Optimized DataLoader settings\n","batch_size = 6  # Increased from 4 to 16\n","num_workers = 8  # Reduced from 12 to 8 to balance CPU usage\n","prefetch_factor = 4  # Added prefetch_factor for data preloading\n","\n","# Create DataLoaders\n","train_loader = DataLoader(\n","    combined_train_dataset,\n","    batch_size=batch_size,\n","    shuffle=True,\n","    num_workers=num_workers,\n","    pin_memory=True,\n","    prefetch_factor=prefetch_factor,\n","    persistent_workers=True  # Keeps workers alive for faster data loading\n",")\n","val_loader = DataLoader(\n","    test_dataset,\n","    batch_size=batch_size,\n","    shuffle=False,\n","    num_workers=num_workers,\n","    pin_memory=True,\n","    prefetch_factor=prefetch_factor,\n","    persistent_workers=True\n",")\n","\n","# Step 5: Define Optimizer, Scheduler, and Mixed Precision Scaler\n","learning_rate = 1e-5\n","num_epochs = 20       # Set epochs: 20 or 30\n","weight_decay = 1e-4\n","optimizer = AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n","total_steps = len(train_loader) * num_epochs\n","scheduler = get_linear_schedule_with_warmup(\n","    optimizer,\n","    num_warmup_steps=int(0.1 * total_steps),  # 10% of steps for warm-up\n","    num_training_steps=total_steps\n",")\n","scaler = GradScaler()  # Initialized for mixed precision\n","\n","# Step 6: Training Loop with Validation and Best Model Saving\n","best_val_accuracy = 0.0  # Initialize best validation accuracy\n","best_model_path = \"best_model_MEA_even_frames_20_epochs.pth\"  # Filepath to save the best model, for 30 epochs: \"best_model_MEA_even_frames_20_epochs.pth\"\n","\n","for epoch in range(num_epochs):\n","\n","    # Log learning rate at the start of each epoch\n","    current_lrs = [param_group['lr'] for param_group in optimizer.param_groups]\n","    print(f\"Epoch {epoch + 1}/{num_epochs} - Current Learning Rates: {current_lrs}\")\n","\n","    # Training Phase\n","    model.train()\n","    epoch_loss = 0.0\n","    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{num_epochs} - Training\")\n","\n","    for batch in progress_bar:\n","        pixel_values = batch['pixel_values'].to(device, non_blocking=True)\n","        labels = batch['labels'].to(device, non_blocking=True)\n","        optimizer.zero_grad()\n","\n","        with autocast():  # Enables mixed precision\n","            outputs = model(pixel_values=pixel_values, labels=labels)\n","            loss = outputs.loss\n","\n","        scaler.scale(loss).backward()  # Scales loss for mixed precision\n","        scaler.step(optimizer)\n","        scaler.update()\n","        scheduler.step()\n","\n","        epoch_loss += loss.item()\n","        progress_bar.set_postfix({'loss': loss.item()})\n","\n","    avg_loss = epoch_loss / len(train_loader)\n","    print(f\"Epoch {epoch + 1} Average Training Loss: {avg_loss:.4f}\")\n","\n","    # Validation Phase\n","    model.eval()\n","    total_correct = 0\n","    total_samples = 0\n","    epoch_val_loss = 0.0\n","    with torch.no_grad():\n","        progress_bar_val = tqdm(val_loader, desc=f\"Epoch {epoch + 1}/{num_epochs} - Validation\")\n","        for batch in progress_bar_val:\n","            pixel_values = batch['pixel_values'].to(device, non_blocking=True)\n","            labels = batch['labels'].to(device, non_blocking=True)\n","\n","            with autocast():  # Ensures consistency in mixed precision\n","                outputs = model(pixel_values=pixel_values, labels=labels)\n","                loss = outputs.loss\n","                logits = outputs.logits\n","\n","            epoch_val_loss += loss.item()\n","            predictions = torch.argmax(logits, dim=-1)\n","            total_correct += (predictions == labels).sum().item()\n","            total_samples += labels.size(0)\n","\n","    val_accuracy = total_correct / total_samples\n","    avg_val_loss = epoch_val_loss / len(val_loader)\n","    print(f\"Epoch {epoch + 1} Validation Loss: {avg_val_loss:.4f} | Validation Accuracy: {val_accuracy:.4f}\")\n","\n","    # Check if this is the best model so far\n","    if val_accuracy > best_val_accuracy:\n","        best_val_accuracy = val_accuracy\n","        torch.save({\n","            'epoch': epoch + 1,\n","            'model_state_dict': model.state_dict(),\n","            'optimizer_state_dict': optimizer.state_dict(),\n","            'scheduler_state_dict': scheduler.state_dict(),\n","            'val_accuracy': val_accuracy,\n","        }, best_model_path)\n","        print(f\"✅ Best model improved to {val_accuracy:.4f}. Saved to {best_model_path}\")\n","    else:\n","        print(f\"➖ Validation accuracy did not improve from {best_val_accuracy:.4f}\")\n","\n","print(\"Training Complete!\")\n","print(f\"Best Validation Accuracy: {best_val_accuracy:.4f}\")"],"metadata":{"id":"0H2nZfSuSoAi"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Model Assessment"],"metadata":{"id":"zGF0WixIUG8B"}},{"cell_type":"code","source":["import torch\n","import numpy as np\n","\n","# Load the full checkpoint\n","checkpoint = torch.load(\"best_model_MEA_even_frames_20_epochs.pth\", map_location=device)        # or \"best_model_MEA_even_frames_30_epochs.pth\" for 30 epochs\n","\n","# Extract the model's state dictionary from the checkpoint\n","model_state_dict = checkpoint['model_state_dict']\n","\n","# Load the state dictionary into the model\n","model.load_state_dict(model_state_dict)\n","\n","# Put the model in evaluation mode\n","model.eval()\n","\n","# Assuming model is already defined and loaded with model.load_state_dict\n","model.eval()\n","\n","correct_top1 = 0\n","correct_top5 = 0\n","correct_top10 = 0\n","total = 0\n","\n","# No gradient needed during evaluation\n","with torch.no_grad():\n","    for batch in val_loader:\n","        # Extract and move data to device\n","        videos = batch['pixel_values'].squeeze(1).to(device)\n","        labels = batch['labels'].to(device)\n","\n","        # Forward pass through the model\n","        outputs = model(pixel_values=videos)\n","        logits = outputs.logits\n","\n","        # Get top-k predictions\n","        _, top1_preds = logits.topk(1, dim=-1)  # Top-1 predictions\n","        _, top5_preds = logits.topk(5, dim=-1)  # Top-5 predictions\n","        _, top10_preds = logits.topk(10, dim=-1)  # Top-10 predictions\n","\n","        # Update Top-1 correct count\n","        correct_top1 += (top1_preds.squeeze() == labels).sum().item()\n","\n","        # Update Top-5 correct count\n","        correct_top5 += sum([labels[i] in top5_preds[i] for i in range(len(labels))])\n","\n","        # Update Top-10 correct count\n","        correct_top10 += sum([labels[i] in top10_preds[i] for i in range(len(labels))])\n","\n","        # Update total sample count\n","        total += labels.size(0)\n","\n","# Calculate accuracies\n","top1_accuracy = correct_top1 / total\n","top5_accuracy = correct_top5 / total\n","top10_accuracy = correct_top10 / total\n","\n","# Print accuracies\n","print(f\"Top-1 Accuracy: {top1_accuracy:.4f}\")\n","print(f\"Top-5 Accuracy: {top5_accuracy:.4f}\")\n","print(f\"Top-10 Accuracy: {top10_accuracy:.4f}\")\n","\n"],"metadata":{"id":"5dRD_6D_UJYI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## VideoMEA: 30 Epochs with Censecutive Frames"],"metadata":{"id":"ELMDJLCtURyk"}},{"cell_type":"code","source":["# Step 1: Load Annotations and Determine Number of Classes\n","annotation_file = 'nslt_100.json'\n","root_dir = 'WLASL100_videos'\n","with open(annotation_file, \"r\") as f:\n","    annotations = json.load(f)\n","\n","# Ensure that gloss_to_index contains mappings for all unique labels in the dataset\n","all_actions = set(data[\"action\"][0] for data in annotations.values())\n","gloss_to_index = {action: idx for idx, action in enumerate(sorted(all_actions))}\n","num_classes = len(gloss_to_index)\n","\n","# Create the inverse mapping from index to gloss\n","index_to_gloss = {idx: gloss for gloss, idx in gloss_to_index.items()}\n","\n","# Step 2: Initialize Model and Processor\n","pretrained_weights = \"MCG-NJU/videomae-base-finetuned-kinetics\"\n","processor = AutoProcessor.from_pretrained(pretrained_weights)\n","model = VideoMAEForVideoClassification.from_pretrained(\n","    pretrained_weights,\n","    num_labels=num_classes,  # Ensure model output fits dataset\n","    ignore_mismatched_sizes=True\n",")\n","\n","# Modify the model's classification head\n","model.classifier = nn.Sequential(\n","    nn.Dropout(p=0.5),\n","    nn.Linear(model.config.hidden_size, num_classes)  # Change to match num_classes\n",")\n","\n","# Detect device to execute model\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model.to(device)\n","\n","# Step 3: Implement Dataset Class\n","class VideoDataset(Dataset):\n","    def __init__(self, annotation_file, root_dir, processor, num_frames, transform, subset):\n","        self.annotation_file = annotation_file\n","        self.root_dir = root_dir\n","        self.processor = processor\n","        self.num_frames = num_frames\n","        self.transform = transform\n","        self.subset = subset\n","\n","        # Load annotations\n","        with open(self.annotation_file, \"r\") as f:\n","            self.annotations = json.load(f)\n","\n","        # Filter videos by subset\n","        self.video_list = [\n","            vid for vid, data in self.annotations.items() if data.get(\"subset\") == self.subset\n","        ]\n","        self.labels = {vid: data[\"action\"][0] for vid, data in self.annotations.items()}\n","\n","    def __len__(self):\n","        return len(self.video_list)\n","\n","    def __getitem__(self, idx):\n","        video_id = self.video_list[idx]\n","        label = gloss_to_index[self.labels[video_id]]\n","        video_path = os.path.join(self.root_dir, f\"{video_id}.mp4\")\n","        # Load video frames\n","        frames = self.load_video_frames(video_path)\n","        # Fallback for videos with no frames\n","        if not frames:\n","            print(f\"Warning: No frames loaded for video {video_id}.\")\n","            frames = [np.zeros((224, 224, 3), dtype=np.uint8) for _ in range(self.num_frames)]\n","        # Apply transformations if any\n","        if self.transform:\n","            frames = self.apply_transforms(frames)\n","        # Process frames with processor\n","        inputs = self.processor(frames, return_tensors=\"pt\", do_rescale=False)[\"pixel_values\"]\n","        return {\"pixel_values\": inputs.squeeze(0), \"labels\": torch.tensor(label)}\n","\n","    def load_video_frames(self, video_path):\n","        cap = cv2.VideoCapture(video_path)\n","        if not cap.isOpened():\n","            print(f\"Error: Cannot open video file {video_path}.\")\n","            return []\n","        frames = []\n","        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n","        start_frame = 0 if total_frames < self.num_frames else random.randint(0, max(total_frames - self.num_frames, 0))\n","        cap.set(cv2.CAP_PROP_POS_FRAMES, start_frame)\n","\n","        for _ in range(self.num_frames):\n","            ret, frame = cap.read()\n","            if not ret:\n","                break\n","            h, w, _ = frame.shape\n","            if min(w, h) < 226:\n","                scale = 226 / min(w, h)\n","                frame = cv2.resize(frame, None, fx=scale, fy=scale)\n","            if max(w, h) > 256:\n","                scale = 256 / max(w, h)\n","                frame = cv2.resize(frame, None, fx=scale, fy=scale)\n","            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n","            frames.append(frame)\n","        cap.release()\n","\n","        # Pad frames if fewer than num_frames\n","        if len(frames) < self.num_frames:\n","            pad_choice = random.random()\n","            if pad_choice < 0.5 and frames:\n","                pad_frame = frames[0]\n","                while len(frames) < self.num_frames:\n","                    frames.insert(0, pad_frame)\n","            elif frames:\n","                pad_frame = frames[-1]\n","                while len(frames) < self.num_frames:\n","                    frames.append(pad_frame)\n","\n","        # In case frames were empty\n","        while len(frames) < self.num_frames:\n","            frames.append(np.zeros((224, 224, 3), dtype=np.uint8))\n","        return frames\n","\n","    # consistent transformation for a video\n","    def apply_transforms(self, frames):\n","        \"\"\"\n","        Apply consistent transforms to all frames in a video.\n","        \"\"\"\n","        if self.subset == \"train\" or self.subset == \"val\":\n","            # Generate a single random crop and flip decision for the entire video\n","            crop_transform = transforms.RandomCrop(224)\n","            flip_transform = transforms.RandomHorizontalFlip(p=0.5)\n","            random_flip = random.random() < 0.5  # Determine if flipping is applied\n","\n","            # Define a manual crop region using RandomCrop's get_params\n","            first_frame = transforms.ToPILImage()(frames[0])\n","            i, j, h, w = transforms.RandomCrop.get_params(first_frame, output_size=(224, 224))\n","\n","            def consistent_transform(frame):\n","                frame = transforms.ToPILImage()(frame)\n","                frame = transforms.functional.crop(frame, i, j, h, w)  # Apply the same crop\n","                if random_flip:\n","                    frame = transforms.functional.hflip(frame)  # Apply the same flip\n","                return transforms.ToTensor()(frame)\n","\n","            # Apply the consistent transforms to all frames\n","            return [consistent_transform(frame) for frame in frames]\n","        else:  # For validation and testing\n","            pil_transforms = transforms.Compose([\n","                transforms.ToPILImage(),\n","                transforms.CenterCrop(224),\n","                transforms.ToTensor(),\n","            ])\n","            return [pil_transforms(frame) for frame in frames]\n","\n","# Step 4: Initialize Datasets and DataLoaders with Combined Training and Validation\n","# Create separate datasets\n","train_dataset = VideoDataset(\n","    annotation_file=annotation_file,\n","    root_dir=root_dir,\n","    processor=processor,\n","    num_frames=16,\n","    transform=True, # handled by transfrom function\n","    subset=\"train\"\n",")\n","val_dataset = VideoDataset(\n","    annotation_file=annotation_file,\n","    root_dir=root_dir,\n","    processor=processor,\n","    num_frames=16,\n","    transform=True, # handled by transfrom function\n","    subset=\"val\"\n",")\n","test_dataset = VideoDataset(\n","    annotation_file=annotation_file,\n","    root_dir=root_dir,\n","    processor=processor,\n","    num_frames=16,\n","    transform=True, # handled by transfrom function\n","    subset=\"test\"\n",")\n","\n","# Combine train and val datasets for training\n","combined_train_dataset = ConcatDataset([train_dataset, val_dataset])\n","\n","# Optimized DataLoader settings\n","batch_size = 6  # Increased from 4 to 16\n","num_workers = 8  # Reduced from 12 to 8 to balance CPU usage\n","prefetch_factor = 4  # Added prefetch_factor for data preloading\n","\n","# Create DataLoaders\n","train_loader = DataLoader(\n","    combined_train_dataset,\n","    batch_size=batch_size,\n","    shuffle=True,\n","    num_workers=num_workers,\n","    pin_memory=True,\n","    prefetch_factor=prefetch_factor,\n","    persistent_workers=True  # Keeps workers alive for faster data loading\n",")\n","val_loader = DataLoader(\n","    test_dataset,\n","    batch_size=batch_size,\n","    shuffle=False,\n","    num_workers=num_workers,\n","    pin_memory=True,\n","    prefetch_factor=prefetch_factor,\n","    persistent_workers=True\n",")\n","\n","# Step 5: Define Optimizer, Scheduler, and Mixed Precision Scaler\n","learning_rate = 1e-5\n","num_epochs = 20\n","weight_decay = 1e-4\n","optimizer = AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n","total_steps = len(train_loader) * num_epochs\n","scheduler = get_linear_schedule_with_warmup(\n","    optimizer,\n","    num_warmup_steps=int(0.1 * total_steps),  # 10% of steps for warm-up\n","    num_training_steps=total_steps\n",")\n","scaler = GradScaler()  # Initialized for mixed precision\n","\n","# Step 6: Training Loop with Validation and Best Model Saving\n","best_val_accuracy = 0.0  # Initialize best validation accuracy\n","best_model_path = \"best_model_MEA_consec_frames_20_epochs.pth\"  # Filepath to save the best model\n","\n","for epoch in range(num_epochs):\n","\n","    # Log learning rate at the start of each epoch\n","    current_lrs = [param_group['lr'] for param_group in optimizer.param_groups]\n","    print(f\"Epoch {epoch + 1}/{num_epochs} - Current Learning Rates: {current_lrs}\")\n","\n","    # Training Phase\n","    model.train()\n","    epoch_loss = 0.0\n","    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{num_epochs} - Training\")\n","\n","    for batch in progress_bar:\n","        pixel_values = batch['pixel_values'].to(device, non_blocking=True)\n","        labels = batch['labels'].to(device, non_blocking=True)\n","        optimizer.zero_grad()\n","\n","        with autocast():  # Enables mixed precision\n","            outputs = model(pixel_values=pixel_values, labels=labels)\n","            loss = outputs.loss\n","\n","        scaler.scale(loss).backward()  # Scales loss for mixed precision\n","        scaler.step(optimizer)\n","        scaler.update()\n","        scheduler.step()\n","\n","        epoch_loss += loss.item()\n","        progress_bar.set_postfix({'loss': loss.item()})\n","\n","    avg_loss = epoch_loss / len(train_loader)\n","    print(f\"Epoch {epoch + 1} Average Training Loss: {avg_loss:.4f}\")\n","\n","    # Validation Phase\n","    model.eval()\n","    total_correct = 0\n","    total_samples = 0\n","    epoch_val_loss = 0.0\n","    with torch.no_grad():\n","        progress_bar_val = tqdm(val_loader, desc=f\"Epoch {epoch + 1}/{num_epochs} - Validation\")\n","        for batch in progress_bar_val:\n","            pixel_values = batch['pixel_values'].to(device, non_blocking=True)\n","            labels = batch['labels'].to(device, non_blocking=True)\n","\n","            with autocast():  # Ensures consistency in mixed precision\n","                outputs = model(pixel_values=pixel_values, labels=labels)\n","                loss = outputs.loss\n","                logits = outputs.logits\n","\n","            epoch_val_loss += loss.item()\n","            predictions = torch.argmax(logits, dim=-1)\n","            total_correct += (predictions == labels).sum().item()\n","            total_samples += labels.size(0)\n","\n","    val_accuracy = total_correct / total_samples\n","    avg_val_loss = epoch_val_loss / len(val_loader)\n","    print(f\"Epoch {epoch + 1} Validation Loss: {avg_val_loss:.4f} | Validation Accuracy: {val_accuracy:.4f}\")\n","\n","    # Check if this is the best model so far\n","    if val_accuracy > best_val_accuracy:\n","        best_val_accuracy = val_accuracy\n","        torch.save({\n","            'epoch': epoch + 1,\n","            'model_state_dict': model.state_dict(),\n","            'optimizer_state_dict': optimizer.state_dict(),\n","            'scheduler_state_dict': scheduler.state_dict(),\n","            'val_accuracy': val_accuracy,\n","        }, best_model_path)\n","        print(f\"✅ Best model improved to {val_accuracy:.4f}. Saved to {best_model_path}\")\n","    else:\n","        print(f\"➖ Validation accuracy did not improve from {best_val_accuracy:.4f}\")\n","\n","print(\"Training Complete!\")\n","print(f\"Best Validation Accuracy: {best_val_accuracy:.4f}\")"],"metadata":{"id":"BzKl4wvBUI0s"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Model Assessment"],"metadata":{"id":"L1h6__VlVwnq"}},{"cell_type":"code","source":["import torch\n","import numpy as np\n","\n","# Load the full checkpoint\n","checkpoint = torch.load(\"best_model_MEA_consec_frames_20_epochs.pth\", map_location=device)\n","\n","# Extract the model's state dictionary from the checkpoint\n","model_state_dict = checkpoint['model_state_dict']\n","\n","# Load the state dictionary into the model\n","model.load_state_dict(model_state_dict)\n","\n","# Put the model in evaluation mode\n","model.eval()\n","\n","# Assuming model is already defined and loaded with model.load_state_dict\n","model.eval()\n","\n","correct_top1 = 0\n","correct_top5 = 0\n","correct_top10 = 0\n","total = 0\n","\n","# No gradient needed during evaluation\n","with torch.no_grad():\n","    for batch in val_loader:\n","        # Extract and move data to device\n","        videos = batch['pixel_values'].squeeze(1).to(device)\n","        labels = batch['labels'].to(device)\n","\n","        # Forward pass through the model\n","        outputs = model(pixel_values=videos)\n","        logits = outputs.logits\n","\n","        # Get top-k predictions\n","        _, top1_preds = logits.topk(1, dim=-1)  # Top-1 predictions\n","        _, top5_preds = logits.topk(5, dim=-1)  # Top-5 predictions\n","        _, top10_preds = logits.topk(10, dim=-1)  # Top-10 predictions\n","\n","        # Update Top-1 correct count\n","        correct_top1 += (top1_preds.squeeze() == labels).sum().item()\n","\n","        # Update Top-5 correct count\n","        correct_top5 += sum([labels[i] in top5_preds[i] for i in range(len(labels))])\n","\n","        # Update Top-10 correct count\n","        correct_top10 += sum([labels[i] in top10_preds[i] for i in range(len(labels))])\n","\n","        # Update total sample count\n","        total += labels.size(0)\n","\n","# Calculate accuracies\n","top1_accuracy = correct_top1 / total\n","top5_accuracy = correct_top5 / total\n","top10_accuracy = correct_top10 / total\n","\n","# Print accuracies\n","print(f\"Top-1 Accuracy: {top1_accuracy:.4f}\")\n","print(f\"Top-5 Accuracy: {top5_accuracy:.4f}\")\n","print(f\"Top-10 Accuracy: {top10_accuracy:.4f}\")\n","\n"],"metadata":{"id":"I2b2hh2CVzD4"},"execution_count":null,"outputs":[]}]}