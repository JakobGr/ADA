{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import random\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoImageProcessor, TimesformerForVideoClassification\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import cv2\n",
    "import json\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 'Apple Silicion' GPU.\n"
     ]
    }
   ],
   "source": [
    "def identify_device():\n",
    "    # look for cuda\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "        print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    # look for MPS (Apple Silicon GPU)\n",
    "    elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
    "        device = torch.device(\"mps\")\n",
    "        print(f\"Using 'Apple Silicion' GPU.\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "        print(f\"No GPU availabe. Using CPU\")\n",
    "    return device\n",
    "\n",
    "device = identify_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dataset class for loading videos and applying transformations\n",
    "class VideoDataset(Dataset):\n",
    "    def __init__(self, annotation_file, root_dir, processor, num_frames=64, subset=\"train\"):\n",
    "        self.annotation_file = annotation_file\n",
    "        self.root_dir = root_dir\n",
    "        self.processor = processor\n",
    "        self.num_frames = num_frames\n",
    "        self.subset = subset\n",
    "\n",
    "        # Load annotations\n",
    "        with open(self.annotation_file, \"r\") as f:\n",
    "            self.annotations = json.load(f)\n",
    "\n",
    "        # Filter videos by subset\n",
    "        self.video_list = [\n",
    "            vid for vid, data in self.annotations.items() if data[\"subset\"] == self.subset\n",
    "        ]\n",
    "        self.labels = {vid: data[\"action\"][0] for vid, data in self.annotations.items()}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.video_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        video_id = self.video_list[idx]\n",
    "        label = self.labels[video_id]\n",
    "        video_path = os.path.join(self.root_dir, f\"{video_id}.mp4\")\n",
    "\n",
    "        # Load video frames\n",
    "        frames = self.load_video_frames(video_path)\n",
    "\n",
    "        # Apply transformations\n",
    "        frames = self.apply_transforms(frames)\n",
    "\n",
    "        # Process frames with processor\n",
    "        inputs = self.processor(frames, return_tensors=\"pt\")[\"pixel_values\"]\n",
    "\n",
    "        return inputs.squeeze(0), torch.tensor(label)\n",
    "\n",
    "    def load_video_frames(self, video_path):\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        if not cap.isOpened():\n",
    "          print(f\"Error: Unable to open video file {video_path}\")\n",
    "          return []\n",
    "        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        frames = []\n",
    "\n",
    "        # Randomly select starting frame\n",
    "        start_frame = random.randint(0, max(0, total_frames - self.num_frames))\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, start_frame)\n",
    "\n",
    "        for _ in range(self.num_frames):\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "\n",
    "            # Resize logic\n",
    "            h, w, c = frame.shape\n",
    "            if min(w, h) < 226:\n",
    "                scale = 226 / min(w, h)\n",
    "                frame = cv2.resize(frame, dsize=(0, 0), fx=scale, fy=scale)\n",
    "            if max(w, h) > 256:\n",
    "                scale = 256 / max(w, h)\n",
    "                frame = cv2.resize(frame, dsize=(0, 0), fx=scale, fy=scale)\n",
    "\n",
    "            # Convert BGR to RGB\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "            # Append to frames list\n",
    "            frames.append(frame)\n",
    "\n",
    "        cap.release()\n",
    "\n",
    "        # Pad frames if fewer than num_frames\n",
    "        if len(frames) < self.num_frames:\n",
    "            pad_frame = frames[0] if random.random() < 0.5 else frames[-1]\n",
    "            if pad_frame is frames[0]:  # Prepend frames\n",
    "                while len(frames) < self.num_frames:\n",
    "                    frames.insert(0, pad_frame)\n",
    "            else:  # Append frames\n",
    "                while len(frames) < self.num_frames:\n",
    "                    frames.append(pad_frame)\n",
    "\n",
    "        return frames\n",
    "\n",
    "    def apply_transforms(self, frames):\n",
    "        \"\"\"\n",
    "        Apply consistent transforms to all frames in a video.\n",
    "        \"\"\"\n",
    "        if self.subset == \"train\" or self.subset == \"val\":\n",
    "            # Generate a single random crop and flip decision for the entire video\n",
    "            crop_transform = transforms.RandomCrop(224)\n",
    "            flip_transform = transforms.RandomHorizontalFlip(p=0.5)\n",
    "            random_flip = random.random() < 0.5  # Determine if flipping is applied\n",
    "\n",
    "            # Define a manual crop region using RandomCrop's get_params\n",
    "            first_frame = transforms.ToPILImage()(frames[0])\n",
    "            i, j, h, w = transforms.RandomCrop.get_params(first_frame, output_size=(224, 224))\n",
    "\n",
    "            def consistent_transform(frame):\n",
    "                frame = transforms.ToPILImage()(frame)\n",
    "                frame = transforms.functional.crop(frame, i, j, h, w)  # Apply the same crop\n",
    "                if random_flip:\n",
    "                    frame = transforms.functional.hflip(frame)  # Apply the same flip\n",
    "                return transforms.ToTensor()(frame)\n",
    "\n",
    "            # Apply the consistent transforms to all frames\n",
    "            return [consistent_transform(frame) for frame in frames]\n",
    "        else:  # For testing\n",
    "            pil_transforms = transforms.Compose([\n",
    "                transforms.ToPILImage(),\n",
    "                transforms.CenterCrop(224),\n",
    "                transforms.ToTensor(),\n",
    "            ])\n",
    "            return [pil_transforms(frame) for frame in frames]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train Last 3 Layers with 64 frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "annotation_file = 'nslt_100.json'\n",
    "root_dir = 'WLASL100_videos'\n",
    "pretrained_weights = \"facebook/timesformer-base-finetuned-k400\"\n",
    "processor = AutoImageProcessor.from_pretrained(pretrained_weights, do_rescale=False)\n",
    "model = TimesformerForVideoClassification.from_pretrained(pretrained_weights, num_labels=100, ignore_mismatched_sizes=True)\n",
    "\n",
    "model.classifier = nn.Sequential(\n",
    "    nn.Dropout(p=0.5),  # Apply 50% dropout\n",
    "    nn.Linear(model.classifier.in_features, model.classifier.out_features)\n",
    ")\n",
    "\n",
    "# Training Dataset (random cropping and flipping)\n",
    "train_dataset = VideoDataset(\n",
    "    annotation_file=annotation_file,\n",
    "    root_dir=root_dir,\n",
    "    processor=processor,\n",
    "    num_frames=64,\n",
    "    subset=\"train\"\n",
    ")\n",
    "\n",
    "# Validation Dataset (random cropping and flipping)\n",
    "val_dataset = VideoDataset(\n",
    "    annotation_file=annotation_file,\n",
    "    root_dir=root_dir,\n",
    "    processor=processor,\n",
    "    num_frames=64,\n",
    "    subset=\"val\"\n",
    ")\n",
    "\n",
    "# Testing Dataset (center cropping)\n",
    "test_dataset = VideoDataset(\n",
    "    annotation_file=annotation_file,\n",
    "    root_dir=root_dir,\n",
    "    processor=processor,\n",
    "    num_frames=64,\n",
    "    subset=\"test\"\n",
    ")\n",
    "\n",
    "# Combine train and val datasets for training as Li et al. (2020)\n",
    "combined_train_dataset = torch.utils.data.ConcatDataset([train_dataset, val_dataset])\n",
    "\n",
    "# DataLoaders\n",
    "train_loader = DataLoader(combined_train_dataset, batch_size=4, shuffle=True, num_workers=12)\n",
    "val_loader = DataLoader(test_dataset, batch_size=4, shuffle=False, num_workers=12)\n",
    "\n",
    "# Freeze all layers\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Unfreeze the last three layers\n",
    "for name, param in model.named_parameters():\n",
    "    if \"encoder.layer.10\" in name or \"encoder.layer.11\" in name or \"classifier\" in name:\n",
    "        param.requires_grad = True\n",
    "\n",
    "# Move model to device\n",
    "model.to(device)\n",
    "\n",
    "print(\"Trainable Parameters:\")\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name)\n",
    "\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.0001, weight_decay=1e-5)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"max\", patience=1, factor=0.3)\n",
    "\n",
    "best_val_accuracy = 0\n",
    "epochs_no_improve = 0\n",
    "epochs = 20\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "\n",
    "    # Use tqdm to create a progress bar for the training loop\n",
    "    with tqdm(total=len(train_loader), desc=\"Training\", unit=\"batch\") as pbar:\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(pixel_values=inputs, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Update the progress bar\n",
    "            pbar.update(1)\n",
    "            pbar.set_postfix({\"Loss\": f\"{loss.item():.4f}\"})\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}, Loss: {total_loss / len(train_loader)}\")\n",
    "    print(f\"Current learning rate: {optimizer.param_groups[0]['lr']}\")\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    with tqdm(total=len(val_loader), desc=\"Validation\", unit=\"batch\") as pbar:\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(pixel_values=inputs)\n",
    "                predictions = outputs.logits.argmax(dim=-1)\n",
    "                total_correct += (predictions == labels).sum().item()\n",
    "                total_samples += labels.size(0)\n",
    "                pbar.update(1)\n",
    "\n",
    "    val_accuracy = total_correct / total_samples\n",
    "    print(f\"Validation Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "    scheduler.step(val_accuracy)\n",
    "\n",
    "    # Check for improvement\n",
    "    if val_accuracy > best_val_accuracy:\n",
    "        best_val_accuracy = val_accuracy\n",
    "        epochs_no_improve = 0\n",
    "        # Save the best model\n",
    "        torch.save(model.state_dict(), \"best_model_TSF_3_layers_64_frames.pt\")\n",
    "        print(\"Validation accuracy improved, saving model.\")\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        print(f\"No improvement for {epochs_no_improve} epochs.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of TimesformerForVideoClassification were not initialized from the model checkpoint at facebook/timesformer-base-finetuned-k400 and are newly initialized because the shapes did not match:\n",
      "- classifier.weight: found shape torch.Size([400, 768]) in the checkpoint and torch.Size([100, 768]) in the model instantiated\n",
      "- classifier.bias: found shape torch.Size([400]) in the checkpoint and torch.Size([100]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "annotation_file = 'nslt_100.json'\n",
    "root_dir = 'WLASL100_videos'\n",
    "pretrained_weights = \"facebook/timesformer-base-finetuned-k400\"\n",
    "processor = AutoImageProcessor.from_pretrained(pretrained_weights, do_rescale=False)\n",
    "model = TimesformerForVideoClassification.from_pretrained(pretrained_weights, num_labels=100, ignore_mismatched_sizes=True)\n",
    "\n",
    "model.classifier = nn.Sequential(\n",
    "    nn.Dropout(p=0.5),  # Apply 50% dropout\n",
    "    nn.Linear(model.classifier.in_features, model.classifier.out_features)\n",
    ")\n",
    "\n",
    "# Training Dataset (random cropping and flipping)\n",
    "train_dataset = VideoDataset(\n",
    "    annotation_file=annotation_file,\n",
    "    root_dir=root_dir,\n",
    "    processor=processor,\n",
    "    num_frames=64,\n",
    "    subset=\"train\"\n",
    ")\n",
    "\n",
    "# Validation Dataset (random cropping and flipping)\n",
    "val_dataset = VideoDataset(\n",
    "    annotation_file=annotation_file,\n",
    "    root_dir=root_dir,\n",
    "    processor=processor,\n",
    "    num_frames=64,\n",
    "    subset=\"val\"\n",
    ")\n",
    "\n",
    "# Testing Dataset (center cropping)\n",
    "test_dataset = VideoDataset(\n",
    "    annotation_file=annotation_file,\n",
    "    root_dir=root_dir,\n",
    "    processor=processor,\n",
    "    num_frames=64,\n",
    "    subset=\"test\"\n",
    ")\n",
    "\n",
    "# Combine train and val datasets for training as Li et al. (2020)\n",
    "combined_train_dataset = torch.utils.data.ConcatDataset([train_dataset, val_dataset])\n",
    "\n",
    "# DataLoaders\n",
    "train_loader = DataLoader(combined_train_dataset, batch_size=4, shuffle=True)\n",
    "val_loader = DataLoader(test_dataset, batch_size=4, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(\"best_model_TSF_3_layers_64_frames.pt\", map_location=device, weights_only=False)) # Load respective model\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "correct_top1 = 0\n",
    "correct_top5 = 0\n",
    "correct_top10 = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for videos, labels in val_loader:\n",
    "        videos = videos.squeeze(1).to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(pixel_values=videos)\n",
    "        logits = outputs.logits\n",
    "        _, top1_preds = logits.topk(1, dim=-1)  # Top-1 predictions\n",
    "        _, top5_preds = logits.topk(5, dim=-1)  # Top-5 predictions\n",
    "        _, top10_preds = logits.topk(10, dim=-1)  # Top-10 predictions\n",
    "\n",
    "        # Update metrics\n",
    "        correct_top1 += (top1_preds.squeeze() == labels).sum().item()\n",
    "        correct_top5 += sum([labels[i] in top5_preds[i] for i in range(len(labels))])\n",
    "        correct_top10 += sum([labels[i] in top10_preds[i] for i in range(len(labels))])\n",
    "        total += labels.size(0)\n",
    "\n",
    "# Calculate accuracies\n",
    "top1_accuracy = correct_top1 / total\n",
    "top5_accuracy = correct_top5 / total\n",
    "top10_accuracy = correct_top10 / total\n",
    "\n",
    "print(f\"Top-1 Accuracy: {top1_accuracy:.4f}\")\n",
    "print(f\"Top-5 Accuracy: {top5_accuracy:.4f}\")\n",
    "print(f\"Top-10 Accuracy: {top10_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16 Frame evenly and all layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update dataset class to sample frames evenly across the video\n",
    "class VideoDataset(Dataset):\n",
    "    def __init__(self, annotation_file, root_dir, processor, num_frames=64, subset=\"train\"):\n",
    "        self.annotation_file = annotation_file\n",
    "        self.root_dir = root_dir\n",
    "        self.processor = processor\n",
    "        self.num_frames = num_frames\n",
    "        self.subset = subset\n",
    "\n",
    "        # Load annotations\n",
    "        with open(self.annotation_file, \"r\") as f:\n",
    "            self.annotations = json.load(f)\n",
    "\n",
    "        # Filter videos by subset\n",
    "        self.video_list = [\n",
    "            vid for vid, data in self.annotations.items() if data[\"subset\"] == self.subset\n",
    "        ]\n",
    "        self.labels = {vid: data[\"action\"][0] for vid, data in self.annotations.items()}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.video_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        video_id = self.video_list[idx]\n",
    "        label = self.labels[video_id]\n",
    "        video_path = os.path.join(self.root_dir, f\"{video_id}.mp4\")\n",
    "\n",
    "        # Load video frames\n",
    "        frames = self.load_video_frames(video_path)\n",
    "\n",
    "        # Apply transformations\n",
    "        frames = self.apply_transforms(frames)\n",
    "\n",
    "        # Process frames with processor\n",
    "        inputs = self.processor(frames, return_tensors=\"pt\")[\"pixel_values\"]\n",
    "\n",
    "        return inputs.squeeze(0), torch.tensor(label)\n",
    "\n",
    "    def load_video_frames(self, video_path):\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        if not cap.isOpened():\n",
    "            print(f\"Error: Cannot open video file {video_path}.\")\n",
    "            return []\n",
    "\n",
    "        frames = []\n",
    "        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "        # Calculate step size to evenly distribute frames\n",
    "        if total_frames > 1:\n",
    "            step = max(1, total_frames // self.num_frames)\n",
    "        else:\n",
    "            step = 1\n",
    "\n",
    "        for i in range(0, step * self.num_frames, step):\n",
    "            cap.set(cv2.CAP_PROP_POS_FRAMES, i)\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            h, w, _ = frame.shape\n",
    "            if min(w, h) < 226:\n",
    "                scale = 226 / min(w, h)\n",
    "                frame = cv2.resize(frame, None, fx=scale, fy=scale)\n",
    "            if max(w, h) > 256:\n",
    "                scale = 256 / max(w, h)\n",
    "                frame = cv2.resize(frame, None, fx=scale, fy=scale)\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            frames.append(frame)\n",
    "\n",
    "        cap.release()\n",
    "\n",
    "        # Handle cases with fewer frames than expected\n",
    "        if len(frames) < self.num_frames:\n",
    "            pad_choice = random.random()\n",
    "            if pad_choice < 0.5 and frames:\n",
    "                pad_frame = frames[0]\n",
    "                while len(frames) < self.num_frames:\n",
    "                    frames.insert(0, pad_frame)\n",
    "            elif frames:\n",
    "                pad_frame = frames[-1]\n",
    "                while len(frames) < self.num_frames:\n",
    "                    frames.append(pad_frame)\n",
    "\n",
    "        # In case frames were empty\n",
    "        while len(frames) < self.num_frames:\n",
    "            frames.append(np.zeros((224, 224, 3), dtype=np.uint8))\n",
    "\n",
    "        return frames\n",
    "\n",
    "    def apply_transforms(self, frames):\n",
    "        \"\"\"\n",
    "        Apply consistent transforms to all frames in a video.\n",
    "        \"\"\"\n",
    "        if self.subset == \"train\" or self.subset == \"val\":\n",
    "            # Generate a single random crop and flip decision for the entire video\n",
    "            crop_transform = transforms.RandomCrop(224)\n",
    "            flip_transform = transforms.RandomHorizontalFlip(p=0.5)\n",
    "            random_flip = random.random() < 0.5  # Determine if flipping is applied\n",
    "\n",
    "            # Define a manual crop region using RandomCrop's get_params\n",
    "            first_frame = transforms.ToPILImage()(frames[0])\n",
    "            i, j, h, w = transforms.RandomCrop.get_params(first_frame, output_size=(224, 224))\n",
    "\n",
    "            def consistent_transform(frame):\n",
    "                frame = transforms.ToPILImage()(frame)\n",
    "                frame = transforms.functional.crop(frame, i, j, h, w)  # Apply the same crop\n",
    "                if random_flip:\n",
    "                    frame = transforms.functional.hflip(frame)  # Apply the same flip\n",
    "                return transforms.ToTensor()(frame)\n",
    "\n",
    "            # Apply the consistent transforms to all frames\n",
    "            return [consistent_transform(frame) for frame in frames]\n",
    "        else:  # For testing\n",
    "            pil_transforms = transforms.Compose([\n",
    "                transforms.ToPILImage(),\n",
    "                transforms.CenterCrop(224),\n",
    "                transforms.ToTensor(),\n",
    "            ])\n",
    "            return [pil_transforms(frame) for frame in frames]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "annotation_file = 'nslt_100.json'\n",
    "root_dir = 'WLASL100_videos'\n",
    "pretrained_weights = \"facebook/timesformer-base-finetuned-k400\"\n",
    "processor = AutoImageProcessor.from_pretrained(pretrained_weights, do_rescale=False)\n",
    "model = TimesformerForVideoClassification.from_pretrained(pretrained_weights, num_labels=100, ignore_mismatched_sizes=True)\n",
    "\n",
    "model.classifier = nn.Sequential(\n",
    "    nn.Dropout(p=0.5),  # Apply 50% dropout\n",
    "    nn.Linear(model.classifier.in_features, model.classifier.out_features)\n",
    ")\n",
    "\n",
    "# Training Dataset (random cropping and flipping)\n",
    "train_dataset = VideoDataset(\n",
    "    annotation_file=annotation_file,\n",
    "    root_dir=root_dir,\n",
    "    processor=processor,\n",
    "    num_frames=16,\n",
    "    subset=\"train\"\n",
    ")\n",
    "\n",
    "# Validation Dataset (random cropping and flipping)\n",
    "val_dataset = VideoDataset(\n",
    "    annotation_file=annotation_file,\n",
    "    root_dir=root_dir,\n",
    "    processor=processor,\n",
    "    num_frames=16,\n",
    "    subset=\"val\"\n",
    ")\n",
    "\n",
    "# Testing Dataset (center cropping)\n",
    "test_dataset = VideoDataset(\n",
    "    annotation_file=annotation_file,\n",
    "    root_dir=root_dir,\n",
    "    processor=processor,\n",
    "    num_frames=16,\n",
    "    subset=\"test\"\n",
    ")\n",
    "\n",
    "# Combine train and val datasets for training as Li et al. (2020)\n",
    "combined_train_dataset = torch.utils.data.ConcatDataset([train_dataset, val_dataset])\n",
    "\n",
    "# DataLoaders\n",
    "train_loader = DataLoader(combined_train_dataset, batch_size=4, shuffle=True, num_workers=12)\n",
    "val_loader = DataLoader(test_dataset, batch_size=4, shuffle=False, num_workers=12)\n",
    "\n",
    "# Make all layers trainable\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# Move model to device\n",
    "model.to(device)\n",
    "\n",
    "print(\"Trainable Parameters:\")\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name)\n",
    "\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.00001, weight_decay=1e-5)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"max\", patience=1, factor=0.3)\n",
    "\n",
    "best_val_accuracy = 0\n",
    "epochs_no_improve = 0\n",
    "epochs = 20\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "\n",
    "    # Use tqdm to create a progress bar for the training loop\n",
    "    with tqdm(total=len(train_loader), desc=\"Training\", unit=\"batch\") as pbar:\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(pixel_values=inputs, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Update the progress bar\n",
    "            pbar.update(1)\n",
    "            pbar.set_postfix({\"Loss\": f\"{loss.item():.4f}\"})\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}, Loss: {total_loss / len(train_loader)}\")\n",
    "    print(f\"Current learning rate: {optimizer.param_groups[0]['lr']}\")\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    with tqdm(total=len(val_loader), desc=\"Validation\", unit=\"batch\") as pbar:\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(pixel_values=inputs)\n",
    "                predictions = outputs.logits.argmax(dim=-1)\n",
    "                total_correct += (predictions == labels).sum().item()\n",
    "                total_samples += labels.size(0)\n",
    "                pbar.update(1)\n",
    "\n",
    "    val_accuracy = total_correct / total_samples\n",
    "    print(f\"Validation Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "    scheduler.step(val_accuracy)\n",
    "\n",
    "    # Check for improvement\n",
    "    if val_accuracy > best_val_accuracy:\n",
    "        best_val_accuracy = val_accuracy\n",
    "        epochs_no_improve = 0\n",
    "        # Save the best model\n",
    "        torch.save(model.state_dict(), \"best_model_TSF_all_layers_16_frames.pt\")\n",
    "        print(\"Validation accuracy improved, saving model.\")\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        print(f\"No improvement for {epochs_no_improve} epochs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(\"best_model_TSF_all_layers_16_frames.pt\", map_location=device, weights_only=False)) # Load respective model\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "correct_top1 = 0\n",
    "correct_top5 = 0\n",
    "correct_top10 = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for videos, labels in val_loader:\n",
    "        videos = videos.squeeze(1).to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(pixel_values=videos)\n",
    "        logits = outputs.logits\n",
    "        _, top1_preds = logits.topk(1, dim=-1)  # Top-1 predictions\n",
    "        _, top5_preds = logits.topk(5, dim=-1)  # Top-5 predictions\n",
    "        _, top10_preds = logits.topk(10, dim=-1)  # Top-10 predictions\n",
    "\n",
    "        # Update metrics\n",
    "        correct_top1 += (top1_preds.squeeze() == labels).sum().item()\n",
    "        correct_top5 += sum([labels[i] in top5_preds[i] for i in range(len(labels))])\n",
    "        correct_top10 += sum([labels[i] in top10_preds[i] for i in range(len(labels))])\n",
    "        total += labels.size(0)\n",
    "\n",
    "# Calculate accuracies\n",
    "top1_accuracy = correct_top1 / total\n",
    "top5_accuracy = correct_top5 / total\n",
    "top10_accuracy = correct_top10 / total\n",
    "\n",
    "print(f\"Top-1 Accuracy: {top1_accuracy:.4f}\")\n",
    "print(f\"Top-5 Accuracy: {top5_accuracy:.4f}\")\n",
    "print(f\"Top-10 Accuracy: {top10_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16 frames evenly and last three layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "annotation_file = 'nslt_100.json'\n",
    "root_dir = 'WLASL100_videos'\n",
    "pretrained_weights = \"facebook/timesformer-base-finetuned-k400\"\n",
    "processor = AutoImageProcessor.from_pretrained(pretrained_weights, do_rescale=False)\n",
    "model = TimesformerForVideoClassification.from_pretrained(pretrained_weights, num_labels=100, ignore_mismatched_sizes=True)\n",
    "\n",
    "model.classifier = nn.Sequential(\n",
    "    nn.Dropout(p=0.5),  # Apply 50% dropout\n",
    "    nn.Linear(model.classifier.in_features, model.classifier.out_features)\n",
    ")\n",
    "\n",
    "# Training Dataset (random cropping and flipping)\n",
    "train_dataset = VideoDataset(\n",
    "    annotation_file=annotation_file,\n",
    "    root_dir=root_dir,\n",
    "    processor=processor,\n",
    "    num_frames=16,\n",
    "    subset=\"train\"\n",
    ")\n",
    "\n",
    "# Validation Dataset (random cropping and flipping)\n",
    "val_dataset = VideoDataset(\n",
    "    annotation_file=annotation_file,\n",
    "    root_dir=root_dir,\n",
    "    processor=processor,\n",
    "    num_frames=16,\n",
    "    subset=\"val\"\n",
    ")\n",
    "\n",
    "# Testing Dataset (center cropping)\n",
    "test_dataset = VideoDataset(\n",
    "    annotation_file=annotation_file,\n",
    "    root_dir=root_dir,\n",
    "    processor=processor,\n",
    "    num_frames=16,\n",
    "    subset=\"test\"\n",
    ")\n",
    "\n",
    "# Combine train and val datasets for training as Li et al. (2020)\n",
    "combined_train_dataset = torch.utils.data.ConcatDataset([train_dataset, val_dataset])\n",
    "\n",
    "# DataLoaders\n",
    "train_loader = DataLoader(combined_train_dataset, batch_size=4, shuffle=True, num_workers=12)\n",
    "val_loader = DataLoader(test_dataset, batch_size=4, shuffle=False, num_workers=12)\n",
    "\n",
    "# Freeze all layers\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Unfreeze the last three layers\n",
    "for name, param in model.named_parameters():\n",
    "    if \"encoder.layer.10\" in name or \"encoder.layer.11\" in name or \"classifier\" in name:\n",
    "        param.requires_grad = True\n",
    "\n",
    "# Move model to device\n",
    "model.to(device)\n",
    "\n",
    "print(\"Trainable Parameters:\")\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name)\n",
    "\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.00001, weight_decay=1e-5)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"max\", patience=1, factor=0.3)\n",
    "\n",
    "best_val_accuracy = 0\n",
    "epochs_no_improve = 0\n",
    "epochs = 20\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "\n",
    "    # Use tqdm to create a progress bar for the training loop\n",
    "    with tqdm(total=len(train_loader), desc=\"Training\", unit=\"batch\") as pbar:\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(pixel_values=inputs, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Update the progress bar\n",
    "            pbar.update(1)\n",
    "            pbar.set_postfix({\"Loss\": f\"{loss.item():.4f}\"})\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}, Loss: {total_loss / len(train_loader)}\")\n",
    "    print(f\"Current learning rate: {optimizer.param_groups[0]['lr']}\")\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    with tqdm(total=len(val_loader), desc=\"Validation\", unit=\"batch\") as pbar:\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(pixel_values=inputs)\n",
    "                predictions = outputs.logits.argmax(dim=-1)\n",
    "                total_correct += (predictions == labels).sum().item()\n",
    "                total_samples += labels.size(0)\n",
    "                pbar.update(1)\n",
    "\n",
    "    val_accuracy = total_correct / total_samples\n",
    "    print(f\"Validation Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "    scheduler.step(val_accuracy)\n",
    "\n",
    "    # Check for improvement\n",
    "    if val_accuracy > best_val_accuracy:\n",
    "        best_val_accuracy = val_accuracy\n",
    "        epochs_no_improve = 0\n",
    "        # Save the best model\n",
    "        torch.save(model.state_dict(), \"best_model_TSF_3_layers_16_frames.pt\")\n",
    "        print(\"Validation accuracy improved, saving model.\")\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        print(f\"No improvement for {epochs_no_improve} epochs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(\"best_model_TSF_3_layers_16_frames\", map_location=device, weights_only=False)) # Load respective model\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "correct_top1 = 0\n",
    "correct_top5 = 0\n",
    "correct_top10 = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for videos, labels in val_loader:\n",
    "        videos = videos.squeeze(1).to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(pixel_values=videos)\n",
    "        logits = outputs.logits\n",
    "        _, top1_preds = logits.topk(1, dim=-1)  # Top-1 predictions\n",
    "        _, top5_preds = logits.topk(5, dim=-1)  # Top-5 predictions\n",
    "        _, top10_preds = logits.topk(10, dim=-1)  # Top-10 predictions\n",
    "\n",
    "        # Update metrics\n",
    "        correct_top1 += (top1_preds.squeeze() == labels).sum().item()\n",
    "        correct_top5 += sum([labels[i] in top5_preds[i] for i in range(len(labels))])\n",
    "        correct_top10 += sum([labels[i] in top10_preds[i] for i in range(len(labels))])\n",
    "        total += labels.size(0)\n",
    "\n",
    "# Calculate accuracies\n",
    "top1_accuracy = correct_top1 / total\n",
    "top5_accuracy = correct_top5 / total\n",
    "top10_accuracy = correct_top10 / total\n",
    "\n",
    "print(f\"Top-1 Accuracy: {top1_accuracy:.4f}\")\n",
    "print(f\"Top-5 Accuracy: {top5_accuracy:.4f}\")\n",
    "print(f\"Top-10 Accuracy: {top10_accuracy:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
